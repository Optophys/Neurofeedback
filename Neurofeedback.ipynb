{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook provides the code used in the  \"Real-time detection of neural oscillation bursts allows behaviourally relevant neurofeedback\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The workflow is as follows:\n",
    "In MATLAB\n",
    "    - Timepoints occured beta events are read out from recorded files.[rwds]\n",
    "    - Timepoints of frametriggers are read out. [frame_times]\n",
    "    and saved into mat.file\n",
    "In this Notebook\n",
    "    -We create a json files for train and eval sets with concanternated lists with paths to flow files\n",
    "    [samples[timepointofsample]]\n",
    "    -Iterate through the data to train a StandardScaler and then a SVM model\n",
    "    -Evaluate our trained models on eval dataset which has not been seen by the model before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import glob,os,json,joblib,imageio,cv2,time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import ks_2samp,ttest_ind\n",
    "from flowutils import flow_to_image,readFlow,draw_flow\n",
    "from sklearn.model_selection import train_test_split as train_test_split\n",
    "from sklearn.decomposition import PCA,IncrementalPCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.stats import  ttest_ind\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "class batchGenerator():\n",
    "    \"\"\"Class to create batches for training/evaluation\"\"\"\n",
    "    def __init__(self,pos_file,neg_file,batch_size_per_class=20,num_epochs=10,sample=None,test=0):\n",
    "        import numpy as np\n",
    "        import json        \n",
    "        self.batch_size=batch_size_per_class\n",
    "        self.num_epochs=num_epochs\n",
    "        self.epoch_counter=0\n",
    "        self.batch_counter=0\n",
    "        with open(pos_file) as fi:\n",
    "            self.pos_data=json.load(fi)\n",
    "        with open(neg_file) as fi:\n",
    "            self.neg_data=json.load(fi)\n",
    "        self.batchesPerEpoch=len(self.pos_data)//self.batch_size\n",
    "        self.sample=sample\n",
    "        self.test=test\n",
    "        if self.test==0:\n",
    "            self.shuffle()\n",
    "    def checkSizes(self):\n",
    "        if len(self.neg_data)==len(self.pos_data):\n",
    "            print('OK')\n",
    "        else:\n",
    "            raise Exception ('Not equal number of pos and neg samples')            \n",
    "    def shuffle(self):        \n",
    "        np.random.shuffle(self.pos_data)\n",
    "        np.random.shuffle(self.neg_data)\n",
    "    def createABatch(self):\n",
    "        data_list={}\n",
    "        pos=self.pos_data[self.batch_counter*self.batch_size:self.batch_counter*self.batch_size+self.batch_size]\n",
    "        neg=self.neg_data[self.batch_counter*self.batch_size:self.batch_counter*self.batch_size+self.batch_size]\n",
    "        self.batch_counter=self.batch_counter+1\n",
    "        if self.batch_counter==self.batchesPerEpoch:        \n",
    "            self.shuffle()\n",
    "            self.batch_counter=0\n",
    "            self.epoch_counter=self.epoch_counter+1\n",
    "            print('New Epoch>shuffling data')\n",
    "        \n",
    "        indexes2shuffle=np.arange(0,self.batch_size*2,1,dtype=np.int16)\n",
    "        np.random.shuffle(indexes2shuffle)\n",
    "        keys=np.zeros((self.batch_size*2,1))\n",
    "        keys[:self.batch_size]=1\n",
    "        all_d=pos+neg\n",
    "        all_d=[all_d[indx] for indx in indexes2shuffle]\n",
    "        keys=[keys[indx] for indx in indexes2shuffle]\n",
    "        return all_d,np.squeeze(np.array(keys))\n",
    "        \n",
    "    def check(self):\n",
    "        self.checkSizes()\n",
    "    def get_specific_sample(self):        \n",
    "        pos=self.pos_data[self.sample]\n",
    "        return pos\n",
    "    \n",
    "def Get_number_evalSamples(file):\n",
    "    \"\"\" Checks how many samples are in given dataset\"\"\"\n",
    "    with open(file) as fi:\n",
    "        a=json.load(fi)\n",
    "    return len(a)\n",
    "\n",
    "def get_neg_time4frames(labels,nroftimes,fs=976.5625,throwfirst=10,shift=500):\n",
    "    \"\"\"Create negative samples randomly but not overlapping with positive samples\"\"\"\n",
    "    neg_times=np.where(labels==0)[0]\n",
    "    neg_times=neg_times[np.int64(throwfirst*fs):]    #throw away first 10 sec due to baseline\n",
    "    neg_times=neg_times[:np.int64(-1*fs)] #throw away last sec   \n",
    "    pos_surr_times=np.reshape(np.tile(np.argwhere(labels==1).T,(shift*2+1,1)).T-np.arange(-shift,shift+1,1),-1)\n",
    "    neg_times=np.setdiff1d(neg_times,pos_surr_times)    \n",
    "    neg_time=np.random.choice(neg_times,nroftimes)/fs \n",
    "    neg_time.sort()\n",
    "    recheck=1\n",
    "    while recheck:\n",
    "        if np.any(np.diff(neg_time)<2):\n",
    "            nr2BEreplaced=np.sum(np.diff(neg_time)<2)\n",
    "            neg_time=neg_time[np.insert(np.diff(neg_time)>2,0,1)]\n",
    "            neg_time=np.insert(neg_time,-1,np.random.choice(neg_times,nr2BEreplaced)/fs)\n",
    "            neg_time.sort()            \n",
    "        else:        \n",
    "            recheck=0                \n",
    "    return neg_time\n",
    "\n",
    "def split_json(name):\n",
    "    \"\"\"Splits dataset into train and eval\"\"\"\n",
    "    files = glob.glob('./json/*'+name)\n",
    "    for f in files:\n",
    "        with open(f, 'r') as fi:\n",
    "            data = json.load(fi)\n",
    "        np.random.shuffle(data) #shuffle for a random train/eval split\n",
    "        data_train = data[:int(round(len(data)*0.8))]\n",
    "        data_test = [x for x in data if x not in data_train]\n",
    "        file_name = os.path.splitext(os.path.basename(f))[0]\n",
    "        with open('./json/splitted2/%s_train.json' % file_name, 'w') as fo:\n",
    "            json.dump(data_train, fo)\n",
    "        with open('./json/splitted2/%s_eval.json' % file_name, 'w') as fo:\n",
    "            json.dump(data_test, fo)\n",
    "            \n",
    "def load_flow(data_list):\n",
    "    \"\"\"load flow files\"\"\"\n",
    "    X=320\n",
    "    Y=240\n",
    "    path2flow_data='/media/deeplearning/BCB24522B244E30E/Neurofeedback/'\n",
    "    flow_data=np.empty((len(data_list),len(data_list[0]),Y,X,2),dtype=np.float32)\n",
    "    for flow_id in range(len(data_list)):\n",
    "        for frame_id,frame_name in enumerate(data_list[flow_id]):\n",
    "            try:\n",
    "                flow_data[flow_id,frame_id,:,:,:]=np.float32(readFlow(path2flow_data+frame_name))[:Y,:X,:]\n",
    "            except:\n",
    "                frame_name = frame_name[:-17] + '%08d-%08d.flo' %((int(frame_name[-10:-4])-1),int(frame_name[-10:-4]))\n",
    "                flow_data[flow_id,frame_id,:,:,:]=np.float32(readFlow(path2flow_data+frame_name))[:Y,:X,:]\n",
    "    return flow_data\n",
    "\n",
    "def FlowNames2PNG(data_list):\n",
    "    \"\"\"Get corresponding png filenames\"\"\"\n",
    "    png_list=list()\n",
    "    for sample in data_list:\n",
    "        time_slice=list()\n",
    "        for time_step in sample:\n",
    "            time_slice.append(time_step[:-11]+'.png')\n",
    "        png_list.append(time_slice)\n",
    "    return png_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell reads the frame and beta-event timepoints from .mat files,\n",
    "calculates corresponding framenumbers around the beta event,\n",
    "creates negative examples,\n",
    "writes samples into json file,\n",
    "splits json into train/eval set,\n",
    "\"\"\" \n",
    "lfp_fs=976.5625 #Sampling frequency of beta_reward frame_times\n",
    "\n",
    "for ratNr in [373]: #iterate over rats\n",
    "    for runId in [1,2,3]: #iterate over kfold splits\n",
    "        #Create jsons for all_training \n",
    "        shiftB=1.3 # time in sec relative to reward when our samples start\n",
    "        frames_for_window=50 # how many frames are in individual samples\n",
    "        \n",
    "        #path to mat files\n",
    "        path2matfiles='/mat_files_beta/'        \n",
    "        allsessions=glob.glob(path2matfiles+'data4python*') #\n",
    "        \n",
    "        #which sessions to include\n",
    "        sessions=['G373_190307_1','G373_190307_2','G373_190308_1','G373_190310_1','G373_190310_2','G373_190311_1','G373_190312_1','G373_190313_1','G373_190314_1']\n",
    "        flow_pos=list()\n",
    "        flow_neg=list()\n",
    "        for session in sessions: #iterate over sessions\n",
    "            sess=[s for s in allsessions if session in s][0] # find the correspondind mat file\n",
    "            print(sess)\n",
    "            mat_file=loadmat(sess)\n",
    "            rwrd_times=mat_file['rwrds']\n",
    "            frame_times=np.squeeze(mat_file['frame_times'])\n",
    "            rwrd_times=rwrd_times[(rwrd_times[:,1] - frame_times[-1])<-1,:] #if there is reward too close to video end\n",
    "            \n",
    "            path2flow_data='/media/deeplearning/'+session+'/'\n",
    "           \n",
    "            # create a vector with 1 where there are betaevents to2used to get negative timepoints\n",
    "            beta_events_vector=np.zeros((np.ceil((rwrd_times[-1,1]+2)*lfp_fs).astype(np.int64)),np.bool)\n",
    "            for rwrd in rwrd_times:\n",
    "                beta_events_vector[(rwrd[0]*lfp_fs).astype(np.int64):(rwrd[1]*lfp_fs).astype(np.int64)]=1\n",
    "            # create neg samples from randomly choose non overlapping timepoints    \n",
    "            neg_times=get_neg_time4frames(beta_events_vector,int(rwrd_times.shape[0]),fs=976.5625,throwfirst=frame_times[0],shift=3000)\n",
    "            neg_times.sort()\n",
    "            \n",
    "            # find the closest frame to the beta_event/neg_sample\n",
    "            frame_nr_pos=list()\n",
    "            frame_nr_neg=list()\n",
    "            for rwrd_time,neg_time in zip(rwrd_times[:,0],neg_times):\n",
    "                frame_nr_pos.append(np.argmin(np.power(frame_times-(rwrd_time-shiftB),2)))\n",
    "                frame_nr_neg.append(np.argmin(np.power(frame_times-neg_time,2)))\n",
    "            \n",
    "            # We create a json file with paths to files but not load them yet into memory.. \n",
    "            \n",
    "            # we create a list of all samples, each sample is a list of all flo files in this sample\n",
    "            for frame_id,frame_nr in enumerate(frame_nr_pos):\n",
    "                f_list=list()\n",
    "                for flow_id in range(frames_for_window):        \n",
    "                    f=frame_nr+flow_id\n",
    "                    flow_file_name=session+'/cam2_all/%06d-%06d.flo' % (f , (f+1))\n",
    "                    png_file_name=session+'/cam2_all/%06d.png' % (f)   \n",
    "                    f_list.append(flow_file_name)\n",
    "                flow_pos.append(f_list)\n",
    "            for frame_id,frame_nr in enumerate(frame_nr_neg):\n",
    "                f_list=list()\n",
    "                for flow_id in range(frames_for_window):        \n",
    "                    f=frame_nr+flow_id\n",
    "                    flow_file_name=session + '/cam2_all/%06d-%06d.flo' % (f , (f+1))\n",
    "                    png_file_name=session+'/cam2_all/%06d.png' % (f)  \n",
    "                    f_list.append(flow_file_name)\n",
    "                flow_neg.append(f_list)\n",
    "        #Save the jsons        \n",
    "        with open('./json/ALL_flow_pos_%d_%0.1f_run%d_rat%d.json'%(frames_for_window,abs(shiftB),runId,ratNr),'w') as fi:\n",
    "            json.dump(flow_pos,fi)\n",
    "        with open('./json/ALL_flow_neg_%d_%0.1f_run%d_rat%d.json'%(frames_for_window,abs(shiftB),runId,ratNr),'w') as fi:\n",
    "            json.dump(flow_neg,fi)            \n",
    "        #Split jsons into train/eval\n",
    "        split_json('ALL_flow_*_%d_%0.1f_run%d_rat%d.json' %(frames_for_window,abs(shiftB),runId,ratNr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have prepared out datasets and are ready to train some SVM\n",
    "We first need to StandardScale the data to improve SVM learning, this is also done in batches due to large datasize\n",
    "then we train the model in batches\n",
    "as a control we use a model trained on data with shuffled identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratNrs=[373] #371 done\n",
    "shiftB=1.3\n",
    "frames_for_window=50\n",
    "for ratNr in ratNrs:\n",
    "    for runId in [1,2,3]:       \n",
    "        print('Working on rat %d run %d' %(ratNr,runId)) \n",
    "        # load the train data(only paths)\n",
    "        pos_file='/home/deeplearning/json/splitted2/ALL_flow_pos_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "        neg_file='/home/deeplearning/json/splitted2/ALL_flow_neg_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "        \n",
    "        clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", fit_intercept=False,tol=1e-3)       \n",
    "        clfS = SGDClassifier(loss=\"hinge\", penalty=\"l2\", fit_intercept=False,tol=1e-3) #SVM for shuffled data\n",
    "\n",
    "        X=320\n",
    "        Y=240\n",
    "        with open(pos_file) as fi:\n",
    "            pos_data=json.load(fi)\n",
    "        import time\n",
    "        \n",
    "        #prepare a data scaler\n",
    "        batch_scaling=70 #batch sizes\n",
    "        batch_training=70\n",
    "        batch_eval=70        \n",
    "        epochs=60# aim for total 10e6 samples\n",
    "        \n",
    "        # create a batchgenerators\n",
    "        gen_scaler=batchGenerator(pos_file,neg_file,batch_size_per_class=batch_scaling)\n",
    "        gen_train=batchGenerator(pos_file,neg_file,batch_size_per_class=batch_training)\n",
    "        gen_eval=batchGenerator(pos_file[:-10]+'eval.json',neg_file[:-10]+'eval.json',batch_size_per_class=batch_eval)\n",
    "        \n",
    "        acc=list()\n",
    "        acc_shuffled=list()\n",
    "        t0=time.time()\n",
    "\n",
    "        #Check if we already have a Scaler trained for this dataset\n",
    "        try:\n",
    "            scaler=joblib.load('./json/Scaler_%d_%0.1f_rat%d.joblib'%(frames_for_window,abs(shiftB),ratNr))\n",
    "            print('Found a Scaler, skip to training')\n",
    "        except FileNotFoundError:\n",
    "            #make a new scaler\n",
    "            print('No Scaler found, fitting one..')\n",
    "            scaler=StandardScaler()\n",
    "            for batch_id in tqdm(range(len(pos_data)//batch_scaling)): \n",
    "                [data_list,_]=gen_scaler.createABatch()\n",
    "                t2=time.time()\n",
    "                train_batch=load_flow(data_list)\n",
    "                print(\"Time loading %d\"%(time.time()-t2))\n",
    "                train_batch=np.reshape(train_batch,(len(data_list)*len(data_list[0]),-1))\n",
    "                scaler.partial_fit(train_batch)   \n",
    "                print(\"Time passed %d\"%(time.time()-t0))\n",
    "            #save the trained scaler\n",
    "            joblib.dump(scaler,'./json/Scaler_%d_%0.1f_rat%d.joblib'%(frames_for_window,abs(shiftB),ratNr))\n",
    "        \n",
    "        t1=time.time()\n",
    "        for batch_id in tqdm(range((len(pos_data)//batch_training)*epochs)): \n",
    "            [data_list,keys]=gen_train.createABatch()\n",
    "            train_batch=load_flow(data_list) #load a batch of data into memory\n",
    "            train_batch=np.reshape(train_batch,(len(data_list)*len(data_list[0]),-1)) #reshape to fit scaler\n",
    "            train_batch=scaler.transform(train_batch)\n",
    "            train_batch=np.reshape(train_batch,(len(data_list),len(data_list[0]),Y,X,-1)) #reshape to fit model \n",
    "            train_batch=np.reshape(train_batch,(train_batch.shape[0],-1)) #liniarize\n",
    "            clf.partial_fit(train_batch,keys,classes=np.unique(keys))  #train model on batch          \n",
    "\n",
    "            keys_shuffled=np.copy(keys)\n",
    "            np.random.shuffle(keys_shuffled) # shuffle the keys for nonsense training\n",
    "            clfS.partial_fit(train_batch,keys_shuffled,classes=np.unique(keys)) #train nonsense\n",
    "            if np.remainder(batch_id,20) ==0: #evaluate the net every 20 batches\n",
    "                [data_list,keys]=gen_eval.createABatch()\n",
    "                train_batch=load_flow(data_list)\n",
    "                train_batch=np.reshape(train_batch,(len(data_list)*len(data_list[0]),-1))\n",
    "                train_batch=scaler.transform(train_batch)\n",
    "                train_batch=np.reshape(train_batch,(len(data_list),len(data_list[0]),Y,X,-1))\n",
    "                train_batch=np.reshape(train_batch,(train_batch.shape[0],-1))\n",
    "                res=clf.predict(train_batch)\n",
    "                res_shuffled=clfS.predict(train_batch)\n",
    "                acc.append(accuracy_score(keys,res))\n",
    "                acc_shuffled.append(accuracy_score(keys,res_shuffled))\n",
    "                print('Step:%d Acc:%f'%(batch_id,accuracy_score(keys,res)))\n",
    "                print('Shuffled Acc:%f'%(accuracy_score(keys,res_shuffled)))\n",
    "        joblib.dump(clf,'./json/Model_%d_%0.1f_run%d_rat%d.joblib'%(frames_for_window,abs(shiftB),runId,ratNr))\n",
    "        joblib.dump(clfS,'./json/ModelS_%d_%0.1f_run%d_rat%d.joblib'%(frames_for_window,abs(shiftB),runId,ratNr))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the models\n",
    "\n",
    "resultsPred={}\n",
    "resultsPred_S={}\n",
    "\n",
    "time_att=list()\n",
    "time_att_all=list()\n",
    "\n",
    "ratNrs=[371,373,206]\n",
    "\n",
    "for ratNr in ratNrs:\n",
    "    all_m=list()\n",
    "    all_m_S=list()\n",
    "    for runId in [100,101,102]:\n",
    "        t0 = time.time()        \n",
    "        X=320\n",
    "        Y=240\n",
    "        pos_file='/home/deeplearning/json/splitted2/ALL_flow_pos_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "        neg_file='/home/deeplearning/json/splitted2/ALL_flow_neg_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "        scaler=joblib.load('./json/Scaler_%d_%0.1f_rat%d.joblib'%(frames_for_window,abs(shiftB),ratNr))\n",
    "        clf=joblib.load('./json/Model_%d_%0.1f_run%d_rat%d.joblib'%(frames_for_window, abs(shiftB), runId, ratNr))\n",
    "        clfS=joblib.load('./json/ModelS_%d_%0.1f_run%d_rat%d.joblib'%(frames_for_window,abs(shiftB),runId,ratNr))\n",
    "\n",
    "        samples2evaluate=Get_number_evalSamples(pos_file[:-10]+'eval.json')\n",
    "        samplesperbatch=samples2evaluate//4    \n",
    "        gen_eval=batchGenerator(pos_file[:-10]+'eval.json',neg_file[:-10]+'eval.json',batch_size_per_class=samplesperbatch,test=1)\n",
    "        time_courses=list()\n",
    "        time_courses_all=list()\n",
    "        spatial_att=list()\n",
    "        spatial_att_shuff=list()\n",
    "        for batch_id in tqdm(range(samples2evaluate//samplesperbatch)):\n",
    "            [data_list,keys]=gen_eval.createABatch()\n",
    "            train_batch=load_flow(data_list)\n",
    "            train_batch=np.reshape(train_batch,(len(data_list)*len(data_list[0]),-1))\n",
    "            train_batch=scaler.transform(train_batch)\n",
    "            train_batch=np.reshape(train_batch,(len(data_list),len(data_list[0]),Y,X,-1))\n",
    "            train_batch=np.reshape(train_batch,(train_batch.shape[0],-1))\n",
    "            res=clf.predict(train_batch)\n",
    "            resS=clfS.predict(train_batch)\n",
    "            print('Acc:%f'%(accuracy_score(keys,res)))\n",
    "            print('Acc:%f'%(accuracy_score(keys,resS)))\n",
    "\n",
    "            all_m.append(accuracy_score(keys,res))\n",
    "            all_m_S.append(accuracy_score(keys,resS))\n",
    "            \n",
    "            attention=train_batch*np.squeeze(clf.coef_)\n",
    "            attention=np.reshape(attention,(len(data_list),len(data_list[0]),Y,X,-1))\n",
    "            \n",
    "            correctly_predPos=np.squeeze(np.argwhere((res==keys)*(res==1)))\n",
    "            time_course_all=np.sum(np.sum(np.sum(np.clip(attention,a_min=0,a_max=None),axis=-1),axis=-1),axis=-1)\n",
    "            time_courses_all.append(time_course_all)\n",
    "            attention=attention[correctly_predPos,:,:,:,:]      \n",
    "            time_course_pos=np.sum(np.sum(np.sum(np.clip(attention,a_min=0,a_max=None),axis=-1),axis=-1),axis=-1)\n",
    "            time_courses.append(time_course_pos)\n",
    "        print('TAcc:%f'%np.mean(all_m))\n",
    "        print('TAcc:%f'%np.mean(all_m_S))        \n",
    "        a=np.concatenate(time_courses,axis=0)\n",
    "        b=np.concatenate(time_courses_all,axis=0)\n",
    "        time_att.append(a)\n",
    "        time_att_all.append(b)\n",
    "        \n",
    "    resultsPred[str(ratNr)]=[np.mean(i) for i in [all_m[0:3], all_m[3:6], all_m[9:]]]\n",
    "    resultsPred_S[str(ratNr)]=[np.mean(i) for i in [all_m_S[0:3], all_m_S[3:6], all_m_S[9:]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results of evaluation\n",
    "plt.figure(18)\n",
    "plt.clf()\n",
    "plt.ylim([0.45,0.68])\n",
    "plt.xlim([-0.2,1.3])\n",
    "ax = plt.gca()\n",
    "ax.plot([1,0.05],[np.mean(resultsPred['206']),np.mean(resultsPred_S['206'])],linestyle='-',marker='^',color=[0.5,0.5,0.5])\n",
    "ax.plot([1,0.05],[np.mean(resultsPred['371']),np.mean(resultsPred_S['371'])],linestyle='-',marker='v',color=[0.5,0.5,0.5])\n",
    "ax.plot([1,0.05],[np.mean(resultsPred['373']),np.mean(resultsPred_S['373'])],linestyle='-',marker='s',color=[0.5,0.5,0.5])\n",
    "ax.plot([1.05,0],[np.mean([resultsPred['206'],resultsPred['371'],resultsPred['373']]),np.mean([resultsPred_S['206'],resultsPred_S['371'],resultsPred_S['373']])],linestyle='-',marker='o',color='k',linewidth=2)\n",
    "plt.errorbar(0,np.mean([resultsPred_S['206'],resultsPred_S['371'],resultsPred_S['373']]),yerr=np.std([np.mean(resultsPred_S['206']),np.mean(resultsPred_S['371']),np.mean(resultsPred_S['373'])])/np.sqrt(3),color='k',capsize=2)\n",
    "plt.errorbar(1.05,np.mean([resultsPred['206'],resultsPred['371'],resultsPred['373']]),yerr=np.std([np.mean(resultsPred['206']),np.mean(resultsPred['371']),np.mean(resultsPred['373'])])/np.sqrt(3),color='k',capsize=2)\n",
    "ax.legend(['Rat1','Rat2','Rat3','Mean'],loc='lower right')\n",
    "ax.set_xticks([0.05,1])\n",
    "ax.set_xticklabels(['Shuffled','Real'])\n",
    "print(ttest_ind(np.reshape([np.mean(resultsPred['206']),np.mean(resultsPred['371']),np.mean(resultsPred['373'])],-1),np.reshape([np.mean(resultsPred_S['206']),np.mean(resultsPred_S['371']),np.mean(resultsPred_S['373'])],-1),equal_var=False))\n",
    "ax.plot([0,1.05],[0.65,0.65],'k',linewidth=0.7)\n",
    "ax.plot([0,0],[0.64,0.65],'k',linewidth=0.7)\n",
    "ax.plot([1.05,1.05],[0.64,0.65],'k',linewidth=0.7)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.text(1.05/2,0.655,'*',fontsize=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for figure 3\n",
    "\n",
    "plt.figure(181,figsize=(11,9))\n",
    "plt.clf()\n",
    "ax1 = plt.subplot2grid((9, 11), (0, 0), colspan=4,rowspan=4)\n",
    "ax2 = plt.subplot2grid((9, 11), (5, 0), colspan=4,rowspan=4)\n",
    "ax3 = plt.subplot2grid((9, 11), (0, 5),colspan=3,rowspan=3)\n",
    "ax4 = plt.subplot2grid((9, 11), (3, 5),colspan=3,rowspan=3)\n",
    "ax5 = plt.subplot2grid((9, 11), (6, 5),colspan=3,rowspan=3)\n",
    "ax6 = plt.subplot2grid((9, 11), (0, 8),colspan=3,rowspan=3)\n",
    "ax7 = plt.subplot2grid((9, 11), (3, 8),colspan=3,rowspan=3)\n",
    "ax8 = plt.subplot2grid((9, 11), (6, 8),colspan=3,rowspan=3)\n",
    "xticks_time=[-1,-0.6,-0.2]\n",
    "y_lims_att=[0.1,1.1]\n",
    "yticks_time=[0.2,0.4,0.6,0.8,1]\n",
    "\n",
    "#Figure1\n",
    "ax1.axes.set_ylim([0.45,0.68])\n",
    "ax1.axes.set_xlim([-0.2,1.3])\n",
    "ax1.plot([1,0.05],[np.mean(resultsPred['206']),np.mean(resultsPred_S['206'])],linestyle='-',marker='^',color=[0.5,0.5,0.5])\n",
    "ax1.plot([1,0.05],[np.mean(resultsPred['371']),np.mean(resultsPred_S['371'])],linestyle='-',marker='v',color=[0.5,0.5,0.5])\n",
    "ax1.plot([1,0.05],[np.mean(resultsPred['373']),np.mean(resultsPred_S['373'])],linestyle='-',marker='s',color=[0.5,0.5,0.5])\n",
    "ax1.plot([1.05,0],[np.mean([resultsPred['206'],resultsPred['371'],resultsPred['373']]),np.mean([resultsPred_S['206'],resultsPred_S['371'],resultsPred_S['373']])],linestyle='-',marker='o',color='k',linewidth=2)\n",
    "ax1.errorbar(0,np.mean([resultsPred_S['206'],resultsPred_S['371'],resultsPred_S['373']]),yerr=np.std([np.mean(resultsPred_S['206']),np.mean(resultsPred_S['371']),np.mean(resultsPred_S['373'])])/np.sqrt(3),color='k',capsize=2)\n",
    "ax1.errorbar(1.05,np.mean([resultsPred['206'],resultsPred['371'],resultsPred['373']]),yerr=np.std([np.mean(resultsPred['206']),np.mean(resultsPred['371']),np.mean(resultsPred['373'])])/np.sqrt(3),color='k',capsize=2)\n",
    "ax1.legend(['Rat1','Rat2','Rat3','Mean'],loc='lower right')\n",
    "ax1.set_xticks([0.05,1])\n",
    "ax1.set_xticklabels(['Shuffled','Real'])\n",
    "ax1.plot([0,1.05],[0.65,0.65],'k',linewidth=0.7)\n",
    "ax1.plot([0,0],[0.64,0.65],'k',linewidth=0.7)\n",
    "ax1.plot([1.05,1.05],[0.64,0.65],'k',linewidth=0.7)\n",
    "ax1.text(1.05/2,0.655,'*',fontsize=13)\n",
    "ax1.set_ylabel('Classification accuracy',fontsize=17)\n",
    "\n",
    "#Figure2\n",
    "t_values=[-(shiftB-i/50)+0.2 for i in range(frames_for_window)]\n",
    "values=np.concatenate([gaussian_filter(t1,(0,2))  for t1 in time_att],axis=0)\n",
    "sem=np.std([np.mean(values[0:3]),np.mean(values[3:6]),np.mean(values[6:])],axis=0)/np.sqrt(3)\n",
    "ax2.plot(t_values,np.mean(values,axis=0)/np.max(np.mean(values,axis=0)),'k')\n",
    "ax2.fill_between(t_values, (np.mean(values,axis=0)+sem)/np.max(np.mean(values,axis=0)), (np.mean(values,axis=0)-sem)/np.max(np.mean(values,axis=0)), facecolor='k', alpha=0.2)\n",
    "slope, intercept, r_value, p_value, std_err=linregress(t_values,np.mean(values,axis=0))\n",
    "regress_vals=[slope*t_values[0]+intercept,slope*t_values[-1]+intercept]\n",
    "ax2.plot([t_values[0],t_values[-1]],[regress_vals[0]/np.max(np.mean(values,axis=0)),regress_vals[-1]/np.max(np.mean(values,axis=0))],'k--')\n",
    "ax2.legend(['Attention','Linear regression','SEM'],loc='upper left')\n",
    "ax2.set_xlabel('Time before burst (s)',fontsize=17)\n",
    "ax2.set_ylabel('Attention (normalized)',fontsize=17)\n",
    "ax2.set_xticks(xticks_time)\n",
    "ax2.set_yticks([0.75,0.8,0.85,0.9,0.95,1])\n",
    "\n",
    "#Figure3 Rat206\n",
    "ratNr=206\n",
    "runId=101\n",
    "if ratNr==373 and runId==102:\n",
    "    runId=103\n",
    "X=320\n",
    "Y=240#215\n",
    "pos_file='/home/deeplearning/json/splitted2/ALL_flow_pos_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "neg_file='/home/deeplearning/json/splitted2/ALL_flow_neg_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "scaler=joblib.load('./json/Scaler_%d_%0.1f_rat%d.joblib'%(frames_for_window,abs(shiftB),ratNr))\n",
    "clf=joblib.load('./json/Model_%d_%0.1f_run%d_rat%d.joblib'%(frames_for_window, abs(shiftB), runId, ratNr))\n",
    "with open(pos_file[:-10]+'eval.json') as fi:\n",
    "    pos_data=json.load(fi)\n",
    "Nr=[idx for idx,i in enumerate(pos_data) if '016842' in i[0]]\n",
    "gen_eval=batchGenerator(pos_file[:-10]+'eval.json',neg_file[:-10]+'eval.json',sample=Nr[0],test=1)\n",
    "data_list=[gen_eval.get_specific_sample()]\n",
    "train_batch=load_flow(data_list)\n",
    "png_list=FlowNames2PNG(data_list)\n",
    "train_batch=np.reshape(train_batch,(len(data_list[0]),-1))\n",
    "train_batch=scaler.transform(train_batch)\n",
    "train_batch=np.reshape(train_batch,(1,len(data_list[0]),Y,X,-1))\n",
    "train_batch=np.reshape(train_batch,(train_batch.shape[0],-1))\n",
    "res=clf.predict(train_batch)\n",
    "attention=train_batch*np.squeeze(clf.coef_)\n",
    "attention=np.reshape(attention,(len(data_list),len(data_list[0]),Y,X,-1))\n",
    "attention=gaussian_filter(attention,(0,2,3,3,0))\n",
    "time_course_all=np.sum(np.sum(np.sum(np.clip(attention,a_min=0,a_max=None),axis=-1),axis=-1),axis=-1)\n",
    "path2flow_data='/media/deeplearning/Neurofeedback/'\n",
    "path2save='/media/deeplearning/temp_images/Attention/'\n",
    "t_values=[-(shiftB-i/50)+0.2 for i in range(50)]\n",
    "\n",
    "time_id=np.argmax(zScore(time_course_all[0]))\n",
    "ax3.plot(t_values,time_course_all[0]/np.max(time_course_all[0]),'k')\n",
    "ax3.plot(t_values[time_id],(time_course_all[0]/np.max(time_course_all[0]))[time_id],'rx')\n",
    "ax3.set_xticks(xticks_time)\n",
    "ax3.axes.set_ylim(y_lims_att)\n",
    "ax3.set_yticks(yticks_time)\n",
    "\n",
    "ax6.imshow(imageio.imread(path2flow_data+png_list[0][time_id]))\n",
    "ax6.imshow(np.squeeze(np.sum(np.clip(attention[0,time_id,:,:,:],a_min=0,a_max=None),axis=-1)),alpha=0.4,cmap='hot')\n",
    "ax6.axis('off')\n",
    "ax6.axes.set_title('Rat1')\n",
    "\n",
    "#rat 371\n",
    "ratNr=371\n",
    "runId=101\n",
    "X=320#298 #cut a useless piece\n",
    "Y=240#215\n",
    "pos_file='/home/deeplearning/json/splitted2/ALL_flow_pos_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "neg_file='/home/deeplearning/json/splitted2/ALL_flow_neg_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "scaler=joblib.load('./json/Scaler_%d_%0.1f_rat%d.joblib'%(frames_for_window,abs(shiftB),ratNr))\n",
    "clf=joblib.load('./json/Model_%d_%0.1f_run%d_rat%d.joblib'%(frames_for_window, abs(shiftB), runId, ratNr))\n",
    "with open(pos_file[:-10]+'eval.json') as fi:\n",
    "    pos_data=json.load(fi)\n",
    "Nr=[idx for idx,i in enumerate(pos_data) if '37039' in i[0]]\n",
    "gen_eval=batchGenerator(pos_file[:-10]+'eval.json',neg_file[:-10]+'eval.json',sample=Nr[0],test=1)\n",
    "data_list=[gen_eval.get_specific_sample()]\n",
    "train_batch=load_flow(data_list)\n",
    "png_list=FlowNames2PNG(data_list)\n",
    "train_batch=np.reshape(train_batch,(len(data_list[0]),-1))\n",
    "train_batch=scaler.transform(train_batch)\n",
    "train_batch=np.reshape(train_batch,(1,len(data_list[0]),Y,X,-1))\n",
    "train_batch=np.reshape(train_batch,(train_batch.shape[0],-1))\n",
    "res=clf.predict(train_batch)\n",
    "attention=train_batch*np.squeeze(clf.coef_)\n",
    "attention=np.reshape(attention,(len(data_list),len(data_list[0]),Y,X,-1))\n",
    "attention=gaussian_filter(attention,(0,2,3,3,0))\n",
    "time_course_all=np.sum(np.sum(np.sum(np.clip(attention,a_min=0,a_max=None),axis=-1),axis=-1),axis=-1)\n",
    "time_id=np.argmax(zScore(time_course_all[0]))\n",
    "ax4.plot(t_values,time_course_all[0]/np.max(time_course_all[0]),'k')\n",
    "ax4.plot(t_values[time_id],(time_course_all[0]/np.max(time_course_all[0]))[time_id],'rx')\n",
    "ax4.set_ylabel('Attention (normalized)',fontsize=17)\n",
    "ax4.set_xticks(xticks_time)\n",
    "ax4.axes.set_ylim(y_lims_att)\n",
    "ax4.set_yticks(yticks_time)\n",
    "\n",
    "ax7.imshow(imageio.imread(path2flow_data+png_list[0][time_id]))\n",
    "ax7.imshow(np.squeeze(np.sum(np.clip(attention[0,time_id,:,:,:],a_min=0,a_max=None),axis=-1)),alpha=0.4,cmap='hot')\n",
    "ax7.axis('off')\n",
    "ax7.axes.set_title('Rat2')\n",
    "ratNr=373\n",
    "runId=100\n",
    "X=320\n",
    "Y=240\n",
    "pos_file='/home/deeplearning/json/splitted2/ALL_flow_pos_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "neg_file='/home/deeplearning/json/splitted2/ALL_flow_neg_%d_%0.1f_run%d_rat%d_train.json'%(frames_for_window,abs(shiftB),runId,ratNr)\n",
    "scaler=joblib.load('./json/Scaler_%d_%0.1f_rat%d.joblib'%(frames_for_window,abs(shiftB),ratNr))\n",
    "clf=joblib.load('./json/Model_%d_%0.1f_run%d_rat%d.joblib'%(frames_for_window, abs(shiftB), runId, ratNr))\n",
    "with open(pos_file[:-10]+'eval.json') as fi:\n",
    "    pos_data=json.load(fi)\n",
    "Nr=[idx for idx,i in enumerate(pos_data) if '87158' in i[0]]\n",
    "gen_eval=batchGenerator(pos_file[:-10]+'eval.json',neg_file[:-10]+'eval.json',sample=Nr[0],test=1)\n",
    "data_list=[gen_eval.get_specific_sample()]\n",
    "train_batch=load_flow(data_list)\n",
    "png_list=FlowNames2PNG(data_list)\n",
    "train_batch=np.reshape(train_batch,(len(data_list[0]),-1))\n",
    "train_batch=scaler.transform(train_batch)\n",
    "train_batch=np.reshape(train_batch,(1,len(data_list[0]),Y,X,-1))\n",
    "train_batch=np.reshape(train_batch,(train_batch.shape[0],-1))\n",
    "res=clf.predict(train_batch)\n",
    "attention=train_batch*np.squeeze(clf.coef_)\n",
    "attention=np.reshape(attention,(len(data_list),len(data_list[0]),Y,X,-1))\n",
    "attention=gaussian_filter(attention,(0,2,3,3,0))\n",
    "time_course_all=np.sum(np.sum(np.sum(np.clip(attention,a_min=0,a_max=None),axis=-1),axis=-1),axis=-1)\n",
    "time_id=np.argmax(zScore(time_course_all[0]))\n",
    "ax5.plot(t_values,time_course_all[0]/np.max(time_course_all[0]),'k')\n",
    "ax5.plot(t_values[time_id],(time_course_all[0]/np.max(time_course_all[0]))[time_id],'rx')\n",
    "ax5.set_xlabel('Time before burst (s)',fontsize=17)\n",
    "ax5.set_xticks(xticks_time)\n",
    "ax5.axes.set_ylim(y_lims_att)\n",
    "ax5.set_yticks(yticks_time)\n",
    "ax8.imshow(imageio.imread(path2flow_data+png_list[0][time_id]))\n",
    "ax8.imshow(np.squeeze(np.sum(np.clip(attention[0,time_id,:,:,:],a_min=0,a_max=None),axis=-1)),alpha=0.4,cmap='hot')\n",
    "ax8.axis('off')\n",
    "ax8.axes.set_title('Rat3')\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.1)\n",
    "plt.text(-1016,-700,'a',fontsize=20,fontweight='bold')\n",
    "plt.text(-439,-700,'c',fontsize=20,fontweight='bold')\n",
    "plt.text(-1016,-140,'b',fontsize=20,fontweight='bold')\n",
    "ax1.text(-0.34,0.508,'chance \\nlevel',fontsize=10,horizontalalignment='center',fontweight='bold')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
